## 《爬虫成长记》

### 一、什么是爬虫

- 通过编写程序模拟浏览器上网，然后让其去互联网上抓取数据的过程

### 二、爬虫的分类

- 1. 通用爬虫：搜索引擎
  2. 聚焦爬虫：根据需求爬取指定内容，而不是整个页面

### 三、哪些语言能爬虫

- php、java、c 、c++、python

### 四、常见反爬机制

- 1. robots.txt 协议：网站遵循该协议开通一些允许爬虫的接口，君子协议

  ```html
  http://www.baidu.com/robots.txt
  ```

  2. User-Agent请求头：请求加上该请求头，伪装成浏览器欺骗服务器

  3. 更换代理IP
  4. 模拟cokie，session
  5. 验证码：云打码平台
  6. 。。。。。

### 五、python爬虫请求模块

- 1. urllib [python内置模块]

     ```python
     # 模块使用
     - 实例
     import urllib.request
     url = 'https://www.baidu.com'
     response = urllib.request.urlopen(url=url)
     data = response.read()
     with open('./text.txt' ,'wb') as f:
         f.write(data)
     
     # 注意点：
     1. url必须是ascii编码，即如有中文需先转码，一般是get请求带有请求数据
         import urllib.parse
         params = {'kw':'中国'} # 需转码
         params = urllib.parse.urlencode(params)
         url = url + params
     2. 通过urllib请求获取的响应数据为字节类型，如果想打印看需要decode()一下
     	data = response.read().decode('utf-8')
     3. 为了应对反爬，伪装UA，故需要自定义请求体
         header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.80 Safari/537.36'}
         request_ = urllib.request.Request(url,headers=header)
         response = urllib.request.urlopen(request_)
     4.如果想将爬取下来的页面直接保存可以使用如下方法
     filename = urllib.request.urlretrieve(url, filename='./2.html')
     5.urlopen返回对象常用方法
         response.getcode()
         response.geturl()
         response.read()
     urllib.request.urlopen(url,data,timeout) # 有data就是post请求
     # 需要手动处理url编码
     # 需要手动处理post请求参数
     # 处理cookie和代理操作繁琐
     ```

- 2. requests模块

     ```python
     # 自动处理url编码，post请求参数，简化cookie和代理操作
     pip3 install requests
     - get请求
     url = "https://www.xxx.com"
     param = {
         'type':5,
         'start':'3'
     }
     header = {
         'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)..'
     }
     proxy = {
         {'http':'10.10.10.10'}
     }
     response = requests.get(url,params=param,headers=header，proxies=proxy)
     data = response.text   # 以字符串接收
     data = response.json() # 以json接收
     data = response.content # 以字节码接收
     ========================================================================
     - data请求
     data = {
         'username':'sun'
         'password':'1111'
     }
     response = requests.post(url,data=data,headers=header,proxies=proxy)
     - 需登陆带cookie，通过session发送请求
     1. 创建session对象
     session = requests.session()
     data = {}
     url = 'http://www.renren.com/login'  # 登录页
     session.post(url,data=data,headers=header,proxies=proxy)
     detial_url = 'http://www.xxx.com/profile'  # 详情页
     response = session.get(url=detial_url, headers=header,proxies=proxy)
     # 如果网站编码不是utf-8，则需要对响应先做转码
     response.encoding = 'utf-8'
     data = response.text
     ========================================================================
     - 开启进程池爬取
     from multiprocessing.dummy import Pool
     import requests
     from fake_useragent import UserAgent # 提供的ua模块
     # 实例化进程池
     pool = Pool()  # 默认cpu个数
     # 假如通过requests请求解析到了一些url，那么就可以运用进程池实现高并发
     request_function = lambda link:request.get(url=link).content
     data = pool.map(request_function, link_list)  # 后面的为url的列表
     # 通过pool进行高并发存储
     def data_save(data):
         with open(filepath, 'wb') as f:
             f.write(data)
     savedata_function = lambada data:data_save(data)
     pool.map(savedata_function, data_list)
     pool.close()   # 关闭进程池
     pool.join()    # 等待所有工作进程结束
     [线程回顾](http://www.cnblogs.com/Eva-J/articles/8253549.html#_label19)
     ```

### 六、python爬虫数据解析方法

- 1. 正则解析

     ```python
     - re模块
     单字符：
     	.  : 除换行符以外所有字符
     	[] : [aoe][a-w] 匹配集合中任意一个字符
         [^] : 除了中括号里的元素
     	\d : 数字[0-9]
     	\D : 非数字
     	\w : 数字、字母、下划线、中文
     	\W : 非\w
     	\s : 所有的空白字符、空格、Tab、换页符
     	\S : 非空白
     数量修饰：
     	* ：>=0
     	+ : >=1
     	? : 0 或1
     	{m} : 固定m次  china{3}
     	{m,} : 至少m次
     	{m,n} : m<=x<=n 次
     边界：
     	$ 
     	^
     分组：
     	(ab)  # 获取括号里的内容
         贪婪匹配： .*    # 默认尽可能多的匹配
         非贪婪匹配： .*?  
     re.I : 忽略大小写
     re.M : 多行匹配
     re.S : 单行匹配
     re.sub(正则，替换内容，字符串)
     re.search(正则，待匹配内容).group()   # 只要找到一个就返回对象，内容通过group()获得，没有就是None
     re.match(正则，待匹配内容).group()    # 从头匹配，匹配到了就返回
     re.findall(正则，待匹配内容)   # 返回列表
     obj = re.compile('^div')  # 将正则表达式编译成一个正则对象，可以重复调用其它的matche等方法
     obj.findall(只需待匹配内容)
     ```

- 2. xpath表达式解析

     ```python
     -- xpath表达式
         - 属性定位
             # 找到class属性值为active的div标签
             //div[@class="active"]
         - 层级&索引定位
             # 找到class属性值为tang的div的直系子标签ul下的第二个子标签li下的直系子标签a
             //div[@class='tang']/ul/li[2]/a
         - 逻辑运算
             # 找到href属性值为空且class属性值为du的a标签
             //a[@href='' and @class='du']
         - 模糊匹配
             //div[contains(@class, 'ng')]   # 包含ng
             //div[starts-with(@class, 'ta')]
         - 取文本
             # / 表示获取某个标签下的文本内容
             # // 表示获取某个标签下的文本内容和所有子标签下的文本内容
             //div[@class='xx']/p[1]/text()
             //div[@class='xx']//text()
         - 取属性
             //div[@class="yyy"]//li[2]/a/@href
     -- 安装   
     	pip3 install lxml
     -- chrome浏览器xpath插件
     	[插件地址](http://chromecj.com/web-development/2018-01/892.html)
     	启动/关闭：ctrl+shift+x
     -- 使用
     	from lxml import etree
         1. 将请求的页面字符串传入，转换成一个etree对象
         # 本地文件：tree = etree.parse(文件名) 	
         2. etree对象调用xpath方法执行表达式，获取指定内容，存放在列表中
         flag_list = tree.xpath('xpath表达式')
         # 网络数据：tree = etree.HTML(网页内容字符串)  
         flag_list = tree.xpath()
         3. xpath的的是放在列表中，里面的元素都是一个个element对象，它可以再次调用xpath
         4. **xpath表达式中的`//div/li[2]` 2表示的式div下的第二个li标签，不是从0开始的！！！**
     ```

- 3. BeautifulSoup解析

     ```python
     -- bs表达式
     	- 根据标签名查找
         	# soup.a	只能找到第一个符合要求的标签
          - 获取属性
         	# soup.a.attrs	获取a所有的属性和属性值，返回一个字典
             # soup.a.attrs['href']  获取href属性   
             # soup.a['href] 上面的简写
          - 获取内容
         	# soup.a.string
             # soup.a.text
             # soup.a.get_text()
             ps: 如果标签里还有标签，string得到的结果为None，另两个可以得到所有标签的文本内容
          - find : 找到第一个符合要求的标签
             # soup.find('a')	找到第一个符合要求的
             # soup.find('a', title='xxx')   找到标签为a且title属性为xxx
          - find_all : 找到所有符合要求的标签
             # soup.find_all('a')
             # soup.find_all(['a', 'div'])  找到所有的a和div标签
             # soup.find_all('a', limit=2)  限制前2个
          - select: 根据选择器选择指定的内容,返回的永远是列表，同xpath一样
         	# soup.select('#active')
             层级选择器：
     	    # soup.select('div .du #le .meme .xixi')  选取下面的所有子，孙子
             # soup.select('div > p > .active')  当前标签的直系儿子一级
        【ps】：select选择器返回永远是列表，需要通过下标索引提取指定的对象   
             
     - 安装
     pip3 install bs4
     pip3 install lxml  # bs4需要使用lxml模块
     - 使用
     from bs4 import BeautifulSoup
     1. 创建beautifulsoup对象，然后对象的方法查找指定内容
     soup = BeautifulSoup(open('本地文件'), 'lxml')
     soup = BeautifulSoup('通过requests模块等响应回的字符串或字符串'，'lxml')
     2. 调用bs的方法匹配指定内容
     res = soup.select('div > ul > li')[2]
     [详细见](https://www.cnblogs.com/sunxiuwen/p/10111431.html)
     ```

- 4. PyQuery模块

     ```python
     - 通过jq语法来获取标签
     -- 使用
     from pyquery import PyQuery as pq
     1. 同xpath、bs4一样先创建一个对象
     dom_str = pq(html字符串)
     dom_str('li') # 选区li标签，通过css标签获取
     [详细见](https://www.cnblogs.com/sunxiuwen/p/10120965.html)
     ```

### 七、验证码处理

- 云打码 [详见](https://github.com/Sunxiuwen2018/study_crawler/tree/master/%E5%80%9F%E5%8A%A9%E7%AC%AC%E4%B8%89%E6%96%B9%E8%AF%86%E5%88%AB%E9%AA%8C%E8%AF%81%E7%A0%81%E8%A7%A3%E5%86%B3%E7%99%BB%E5%BD%95)
- 1. 抓取页面的验证码，解析将验证码图片下载到本地
  2. 将验证码图片提交给三方平台识别，返回验证码图片内容
  3. 注意：识别码必须和要通过验证码的页面一起发送请求

### 八、图片懒加载selenium和phantomJs

- 1. 图片懒加载

     > 网站为了提高体验，节约网络资源，只显示当前窗口内的图片，其它当用户划滚时再加载，而一般通过伪属性如src2，这样第一次加载时，不会再发请求，当图片出现到页面的可视化区域中，通过js改变src属性，再发送请求。
     >
     > 爬取时，可以尝试滑动鼠标检查浏览器响应页面的标签属性

- 2. 模拟浏览器

     2.1 . Selenium 对外提供的接口可以操作浏览器，然后让浏览器完成自动化的操作

     - 会自动打开一个浏览器窗口自动点击页面标签等等

     - 缺点：需要驱动，且需要安装对应驱动的浏览器版本

     - 安装：pip3 install selenium     

     - 下载对应浏览器驱动：（chrome为例）[下载地址](http://chromedriver.storage.googleapis.com/index.html)

     - 基本语法

       ```python
       - 导包 from selenium import webdriver
       - 创建浏览器对象 bro = webdriver.Chrome('驱动path')
       - 选取指定元素进行操作
           find_element_by_id('id名')   根据id找节点
           find_element_by_name()		根据name属性找
           find_element_by_xpath() 	根据xpath查找
           find_element_by_tag_name()	根据标签名查找
           find_element_by_class_name() 根据class名查找
       - 可以选中标签执行事件
           bro.find_element_by_id('type_id').send_keys('小姐姐')  # 向对应的输入框中输入内容
           bro.find_element_by_id('type_id').click() 
           bro.find_element_by_id('type_id').clear()
           bro.find_element_by_name('tj_login').is_displayed()  # 判断标签是否存在,返回布尔值
       - 可以执行js代码
           js = 'window.scrollTo(0,document.body.scrollHeight)'  # 滑动浏览器滑轮
           bro.execute_script(js)
       - 发送请求
           bro.get(url)
       - 获取页面源码
           page_text = bro.page_source
       ```

     - 使用：

       ```python
       from selenium import webdriver
       # 创建浏览器对象
       bro = webdriver.Chrome(r'驱动程序路径')
       # 发送请求
       bro.get('https://www.baidu.com')
       # 如向百度输入框输入关键字查询,点击按钮跳到新页面
       bro.find_element_by_id('kw').send_keys('python')
       bro.find_element_by_id('su').click()
       # 滚动底部
       
       
       ```
